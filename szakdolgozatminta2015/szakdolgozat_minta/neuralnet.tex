%!TEX root = dolgozat.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Neural networks}\label{ch:INTRO}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Artificial neurons}\label{sec:INTRO:neurons}

\subsection{Sigmoid neuron}

The sigmoid neuron takes inputs with values between 0 and 1 and produces one output int the same interval.

[kep a nuronrol]

Weights are assigned to each input, representing their importance in the output of the neuron. Each neuron has a bias that is meant to correct small disorders in the behaviour of the network.

The output of a sigmoid neuron is calcualted with the \ref{eq:1} formula. 

\begin{equation} \label{eq:1}
\sigma(\sum\limits_{i=1}^{n-1} w_{i}*x_{i} + b)
\end{equation}

\begin{equation} \label{eq:2}
\sigma(z) = \frac{1}{1+e^{-z}}
\end{equation}

The function in the \ref{eq:2} formula is called the activation function. Activation functions are ment to  place the output in a given interval. In case of the sigmoid function this interval is between 0 and 1.

A characteristic of the sigmoid neuron is that a small change in the weights produces a small change int the output. This can be a disandvantage as much as an advantage. It is an advantage because when the output is close to the desired outcome it can modify it without steping over it. It is a disadvantage because when the output is terribly wrong, it takes a lot of time to correct it.


\section{Architecture}\label{sec:INTRO:architecture}

I used a netwotk with 625 (25*25) input neurons representing the gray value of each pixel, and 46 (number of road sings categories) output neurons. I experimented with different numbers of hidden layers and various numbers of neurons in them.


\section{Stochastic gradient descent}\label{sec:INTRO:graddesc}

To change the weights and biases based on the deviation of the calcualted output to the expected output I used stochastic gradient descent, a simplified version of gradient descent. Gradient descent calculates the cost based on the the enire training set, while stochastic gradient descent approximates it based on a number of randomly selected data. If the number is large enough the approximation will be more accurate, but since it doesn't work with the whole data set it is faster.

Equation \ref{eq:3} is the quadratic cost function. \textit{n} is the number of training inputs, \textit{y(x)} is the output generated by the network for the x input, and \textit{a} is the desired output for the x input. When the desired and the calculated outputs are close to each other the cost will be small. Since my goal is to approach the calcualted output to the desired one I have to minimise the value of the cost function. For this I introduce the gradient of the cost function, denoted as $\nabla C$ defined with the vector in the \ref{eq:6} formula. \cite{DUMMY:1}

\begin{equation} \label{eq:3}
\ C = \frac{1}{2n}\sum\nolimits_{x} \|y(x)-a(x)\|^2
\end{equation}

$\nabla C$ is the gradient of the cost function C and is defined with the vector in the \ref{eq:6} formula. 

\begin{equation} \label{eq:6}
\ \nabla C = (\frac{\partial C}{\partial w_{i}},\frac{\partial C}{\partial b_{j}})^T
\end{equation}

In order to minimise the cost I repeatedly applied \ref{eq:4} and \ref{eq:5} rules to the weights and biases. $\eta$ is the learning rate. It has to be a small munber, but not too small, because the learning process would be too slow.

\begin{equation} \label{eq:4}
\ w_{i} \rightarrow w_{i} - \eta\frac{\partial C}{\partial w_{i}} 
\end{equation}

\begin{equation} \label{eq:5}
\ b_{j} \rightarrow b_{j} - \eta\frac{\partial C}{\partial b_{j}} 
\end{equation}


\section{Backpropagation}\label{sec:INTRO:backprop}

Backpropagation is used to adapt the weights and biases in the network to a achieve the desired output. 

In equation \ref{eq:7} $z^l$ is the wighted input vector in the $l^{th}$ layer. As can be seen int equation \ref{eq:8} the output vector in a layer is calcualted using the output of the previous layer, thus forwrding the information. After calcualting the output of the network the \ref{eq:9} formula calcualates the error of the final layer, where \textit{L} is the number of layers, and \textit{$\nabla_aC$} is a vector containing the $\frac{\partial C}{\partial a_j^L}$ partial dervates.

The backpropagation is done with the \ref{eq:10} formula for each layer \textit{l} from \textit{L-1} to 2.

\begin{equation} \label{eq:7}
\ z^l = (w^l*a^{l-1} + b^l) 
\end{equation}

\begin{equation} \label{eq:8}
\ a^l = \sigma(z^l) 
\end{equation}

\begin{equation} \label{eq:9}
\ \delta^L = \nabla_aC \circ \sigma'(z^L)
\end{equation}

\begin{equation} \label{eq:10}
\ \delta^l = ((w^{l+1})^T* \delta^{l+1}) \circ \sigma'(z^l)
\end{equation}